.. _api:

API
===

Scrapyd原有 API 保留，这里列出新增加的API资源及信息


schedulelist.json
-----------------

用于获取爬虫调用情况,如::

    invoked_spider - 被调用过的爬虫

    invoked_spider - 未被调用过的爬虫

    most_record - 被调用次数最多的爬虫名称及次数

* 支持的请求方式: ``GET``

请求示例::

    curl http://localhost:6800/schedulelist.json

响应示例::

    {"node_name": "node-name", "status": "ok", "invoked_spider": ["tips", "smtb"], "un_invoked_spider": ["lagou"], "most_record": ["tips", 3]}




runtimestats.json
---------------

爬虫运行时间统计,如::

    average - 所有爬虫的平均运行时长

    shortest - 爬虫运行的最短时间

    longest - 爬虫运行的最长时间

* 支持的请求方式: ``GET``

请求示例::

    curl http://localhost:6800/runtimestats.json

响应示例::

    {"node_name": "node-name", "status": "ok", "average": "0:00:04", "shortest": "0:00:01", "longest": "0:00:12"}


.. note:: Scrapyd uses the `distutils LooseVersion`_ to interpret the version numbers you provide.

The latest version for a project will be used by default whenever necessary.

schedule.json_ and listspiders.json_ allow you to explicitly set the desired project version.

.. _distutils LooseVersion: http://epydoc.sourceforge.net/stdlib/distutils.version.LooseVersion-class.html

.. _scrapyd-schedule:


psnstats.json
-------------

项目及爬虫数量统计,如::

    project_nums - 项目总数

    spider_nums - 爬虫总数

* 支持的请求方式: ``GET``

请求示例::

    curl http://localhost:6800/psnstats.json

响应示例::

    {"node_name": "node-name", "status": "ok", "project_nums": 2, "spider_nums": 3}


prospider.json
-------------

项目与对应爬虫名以及数量统计,如::

    pro_spider - 结果集

        project - 项目名称

        spider - 项目对应的爬虫名称

* 支持的请求方式: ``GET``

请求示例::

    curl http://localhost:6800/prospider.json

响应示例::

    {"node_name": "node-name", "status": "ok", "pro_spider": [{"project": "Lagous", "spider": "lagou,tips", "number": 2}, {"project": "smtaobao", "spider": "smtb", "number": 1}]}


timerank.json
-------------

爬虫运行时长榜,如::

    ranks - 结果集

        time - 运行时长

        spider - 项目对应的爬虫名称

* 支持的请求方式: ``GET``
* 参数:

  * ``index`` (int, 可选参数) - 排行榜默认取前10条记录，你可以通过index来指定想要的记录数量

请求示例::

    curl http://localhost:6800/timerank.json?index=5

响应示例::

    {"node_name": "node-name", "status": "ok",
    "ranks":[
        {"time": "0:00:52", "spider": "dps"},
        {"time": "0:00:33", "spider": "invorank"},
        {"time": "0:00:18", "spider": "lagou"},
        {"time": "0:00:10", "spider": "smtb"},
        {"time": "0:00:02", "spider": "tips"}
    ]}



invokerank.json
-------------

爬虫调用排行榜,如::

    ranks - 结果集

* 支持的请求方式: ``GET``
* 参数:

  * ``index`` (int, 可选参数) - 排行榜默认取前10条记录，你可以通过index来指定想要的记录数量

请求示例::

    curl http://localhost:6800/invokerank.json?index=5

响应示例::

    {"node_name": "node-name", "status": "ok",
    "ranks":[
        ["tips", 5],
        ["lagou", 5],
        ["smtb", 4]
    ]}


filter.json
---------------

D根据参数按时间范围/项目名称/爬虫名称/运行时长对爬虫运行记录进行筛选过滤。

* 支持的请求方式: ``POST``
* 参数:

  * ``type`` (string, 不可选) - 过滤类型，可选类型参数为：time/project/spider/runtime,以下为每个类型的对应参数::

    time (string, 参数： st, et) - 时间范围，如2018-09-26 00:00:00

    project (string, 参数： project) - 项目名称，如:Alibaba

    spider (string, 参数： spider) - 爬虫名称，如:tmall

    runtime (int, 参数： runtime) - 运行时长(秒)，如:5

Example request::

    $ curl http://localhost:6800/filter.json -d project=myproject

Example response::

    {"status": "ok"}

.. _DOWNLOAD_DELAY: http://doc.scrapy.org/en/latest/topics/settings.html#download-delay
.. _issue 12: https://github.com/scrapy/scrapyd/issues/12
